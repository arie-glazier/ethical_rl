ENVIRONMENT = "environment"
MODEL = "model"
POLICY = "policy"

# arguments
ENVIRONMENT_NAME = "environment_name"
ENVIRONTMENT_WRAPPER = "environment_wrapper"
MODULES = "modules"
CLASSES = "classes"
MODEL_MODULE = "model_module"
POLICY_MODULE = "policy_module"
ALGORITHM_MODULE = "algorithm_module"
REPLAY_BUFFER_MODULE = "replay_buffer_module"
REWARD_MODULE = "reward_module"
MAX_REPLAY_BUFFER_LENGTH = "max_replay_buffer_length"
REPLAY_BUFFER_PRIORITIZATION = "replay_buffer_prioritization"
BATCH_SIZE = "batch_size"
DISCOUNT_FACTOR = "discount_factor"
OPTIMIZER = "optimizer"
LEARNING_RATE = "learning_rate"
LOSS_FUNCTION = "loss_function"
NUMBER_OF_EPISODES = "number_of_episodes"
MAXIMUM_STEP_SIZE = "maximum_step_size"
EPSILON_SCHEDULE_MODULE = "epsilon_schedule_module"
EPSILON_START = "epsilon_start"
EPSILON_END = "epsilon_end"
EPSILON_ANNEAL_PERCENT = "epsilon_anneal_percent"
BUFFER_WAIT_STEPS = "buffer_wait_steps"
TEST_NAME = "test_name"
FULLY_CONNECTED_MODEL_SIZE = "fully_connected_model_size"
TARGET_SYNC_FREQUENCY = "target_sync_frequency"
CLIP_NORM = "clip_norm"
RENDER_TRAINING_STEPS = "render_training_steps"
RESULTS_DESTINATION = "results_destination"

# environments
CONSTRAINT_VIOLATION_COUNT = "constraint_violation_count"
EPISODE_ACTION_HISTORY = "episode_action_history"
YELLOW_COORDINATES = "yellow_coordinates"
YELLOW = "yellow"
TERMINATION_REWARD = "termination_reward"
STEP_REWARD = "step_reward"
MAX_STEPS_PER_EPISODE = "max_steps_per_episode"

# utils
PWD = "pwd"
ARGS = "args"
ROOT_DIRECTORY = "root_directory"
DEPLOY = "deploy"
FILES = "files"
FOLDERS = "folders"
ZIP = "zip"
SCP = "scp"
SSH = "ssh"
HOST = "host"
ARCHIVE_PATH = "archive_path"
PROJECT_NAME = "project_name"
MODULE = "module"
CLASS = "class"

REWARDS = "rewards"
LOSS = "loss"
CONFIG = "config"
HISTORY = "history"
RANDOM_START_POSITION = "random_start_position"